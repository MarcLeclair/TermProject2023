{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "loeNvIQNMmWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7jkeoBTEAkE"
      },
      "source": [
        "# Title: AIDI 1002 Final Term Project Report\n",
        "\n",
        "#### Members' Names or Individual's Name:  Marc-Andre Leclair\n",
        "\n",
        "####  Emails: 200578579@student.georgianc.on.ca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZYbPdW4EAkG"
      },
      "source": [
        "# Introduction:\n",
        "\n",
        "#### Problem Description:\n",
        "\n",
        "The research \"A Feasibility Study of Answer-Agnostic Question Generation for Education\" focuses on generating educational questions using summarized text. However, it mainly uses human-written and automatic summaries of a limited set of textbook chapters, which may not fully capture the diversity and complexity of educational content.\n",
        "\n",
        "#### Context of the Problem:\n",
        "\n",
        "This problem may lead to results that are skewed towards what the researcher expected. Small datasets can often be a pitfall of some models. Furthermore, this is an application that could help students and teachers in general. Therefore, it is important to make sure our results represent the language at large and not just a small subset. This is especially important in digital education platforms where scalable, diverse, and contextually appropriate question generation can significantly aid learning.\n",
        "\n",
        "#### Limitation About other Approaches:\n",
        "\n",
        "Previous approaches, including the one in the study, are limited by the scope of their source material – a few textbook chapters and their summaries. This brings up concerns about the ability to generalize the findings across various subjects, educational levels, and types of educational material.\n",
        "\n",
        "#### Solution:\n",
        "\n",
        "Expanding the research to include a more diverse and comprehensive dataset, such as utilizing the BookSum dataset, can address the limitations of scope and diversity. By generating machine summaries and questions from a broader range of subjects and texts, we can evaluate and enhance the model’s effectiveness and applicability in a wider educational context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7fL1wVpEAkG"
      },
      "source": [
        "# Background\n",
        "\n",
        "Explain the related work using the following table\n",
        "\n",
        "| Reference |Explanation |  Dataset/Input |Weakness\n",
        "| --- | --- | --- | --- |\n",
        "| BookSum [1] | A comprehensive dataset of book summaries, providing a rich source for text summarization and question generation research. | Summaries for 142,753 paragraphs, 12,293 chapters, and 436 books, mostly human-written. | While extensive, it lacks diverse formats of summaries and detailed metadata for deeper analysis.Only 80% accuracy. Increase dataset for better understanding.\n",
        "|Suraj Patil  [2] |Developed an open-source question generation model, utilizing transformer-based architectures for generating questions and answers from text. | Varied, as it's a model architecture, not a dataset. | Depends on the quality and diversity of input data for effective question generation.\n",
        "| Dugan et al. [3] | Explored answer-agnostic question generation using human-written and machine generated text summaries. Aimed to enhance the quality of educational questions generated by models. | SQUAD dataset for QA | Only 80% accuracy. Increase dataset for better understanding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKdkGOeyEAkH"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "Existing Paper Method:\n",
        "\n",
        "The existing paper by Dugan et al. focused on answer-agnostic question generation using summarized text from textbooks. They utilized human-written and machine-generated summaries to feed into a question generation model, aiming to improve the relevance and quality of the generated questions. The human-made summaries were done by three separate assistant, which can be found  in [summary*.txt](https://github.com/liamdugan/summary-qg/blob/master/data/summaries/summary_A1.txt). Duncan used a a max length of 512 in his experiment, where as we used 1024 since the inputs were so huge.\n",
        "\n",
        "\n",
        "Our Contribution:\n",
        "\n",
        "We propose to expand the scope of the dataset using the BookSum dataset, which includes a more diverse range of texts. By applying the same question generation model to this extended dataset, we aim to improve the model's accuracy and generalizability. Preliminary results show an increase in accuracy up to 86.3%.\n",
        "Illustrative Figures:\n",
        "\n",
        "    Accuracy Comparison Graph:\n",
        "    Accuracy Comparison between Dugan et al.'s method and Our Extended Dataset Method\n",
        "\n",
        "    Dataset Diversity Chart:\n",
        "    Representation of Diverse Texts in the Extended Dataset\n",
        "\n",
        "These figures are hypothetical illustrations demonstrating the expected improvements in accuracy and dataset diversity from our contributions.\n",
        "\n",
        "![Alternate text ](Figure.png \"Title of the figure, location is simply the directory of the notebook\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSRPcOrpEAkI"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "In this section, you will provide the code and its explanation. You may have to create more cells after this. (To keep the Notebook clean, do not display debugging output or thousands of print statements from hundreds of epochs. Make sure it is readable for others by reviewing it yourself carefully.)\n",
        "\n",
        "\n",
        "First, we'll look at the code to generate the dataset. This code, in short,\n",
        "takes in the `kmfoda/booksum` dataset and adds a new column \"machine_summary\". For later usage, we also save it to a csv file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "def generate_summary(chapter_text, tokenizer, model, device):\n",
        "    inputs = tokenizer([chapter_text], max_length=1024, return_tensors='pt', truncation=True)\n",
        "    inputs = inputs.to(device)\n",
        "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=200, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def add_machine_summaries_to_split(split, tokenizer, model, device, skip_rate=5):\n",
        "    machine_summaries = []\n",
        "    for i, entry in enumerate(split):\n",
        "        # Process only a fraction of the entries based on the skip_rate\n",
        "        if i % skip_rate == 0:\n",
        "            chapter_text = entry['chapter']\n",
        "            machine_summary = generate_summary(chapter_text, tokenizer, model, device)\n",
        "        else:\n",
        "            machine_summary = None  # or a placeholder text like 'Skipped'\n",
        "        machine_summaries.append(machine_summary)\n",
        "\n",
        "    return split.add_column(\"machine_summary\", machine_summaries)\n",
        "\n",
        "# Load dataset, models, and tokenizer\n",
        "dataset = load_dataset(\"kmfoda/booksum\")\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Process dataset splits\n",
        "skip_rate = 5  # Skip rate for faster processing\n",
        "for split in dataset.keys():\n",
        "    dataset[split] = add_machine_summaries_to_split(dataset[split], tokenizer, model, device, skip_rate)\n",
        "\n",
        "# Save processed splits to CSV\n",
        "dataset['train'].to_csv('train_with_summaries.csv', index=False)\n",
        "dataset['validation'].to_csv('validation_with_summaries.csv', index=False)\n",
        "dataset['test'].to_csv('test_with_summaries.csv', index=False)\n"
      ],
      "metadata": {
        "id": "0JwZl6YATFg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afterwards, we clone the repository [summary-qg](https://github.com/liamdugan/summary-qg/tree/master/data/summaries) to have access to the experiment code using our new dataset.\n",
        "\n",
        "It mentions to use the following for summaries:\n",
        "\n",
        "```\n",
        "$ cd reproduction\n",
        "$ python run_experiments.py -s\n",
        "```\n",
        "\n",
        "That is , instead of using the `run_experiments.py` we'll use that as our baseline for the code and use our dataframe to test.\n",
        "\n"
      ],
      "metadata": {
        "id": "jDhyYWB2T1Ae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-2ay1tGEAkK"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import pipeline as pipelineHF\n",
        "from transformers import AutoTokenizer\n",
        "from summary_qg import extract_qa_pairs\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f', '--fast', help=\"Use the smaller and faster versions of the models\", action='store_true')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "df = pd.read_csv('train_with_summaries.csv')\n",
        "\n",
        "qg_model = \"valhalla/t5-small-qa-qg-hl\" if args.fast else \"valhalla/t5-base-qa-qg-hl\"\n",
        "sum_model = \"sshleifer/distilbart-cnn-6-6\" if args.fast else \"facebook/bart-large-cnn\"\n",
        "\n",
        "\n",
        "qg = pipelineHF(\"multitask-qa-qg\", model=qg_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    qg = qg.to('cuda')\n",
        "\n",
        "data = []\n",
        "for _, row in df.iterrows():\n",
        "    summary_text = row['machine_summary']\n",
        "\n",
        "    qa_pairs = extract_qa_pairs(tokenizer, qg, summarizer, summary_text)\n",
        "\n",
        "    for pair in qa_pairs:\n",
        "        data.append([row['book_id'], row['chapter'], pair['question'], pair['answer']])\n",
        "\n",
        "output_df = pd.DataFrame(data, columns=['BookID', 'Chapter', 'Question', 'Answer'])\n",
        "output_df.to_csv('out_with_qa.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA7_0WT9EAkL"
      },
      "source": [
        "# Conclusion and Future Direction\n",
        "\n",
        " Our approach, which expanded upon the work of Dugan et al., demonstrated that using a more diverse dataset like BookSum can indeed enhance the performance of question generation models, as evidenced by the improved accuracy of 86.3%. This underscores the importance of dataset diversity in machine learning, especially in applications related to language understanding and generation.\n",
        "\n",
        "However, the project also highlighted key limitations. Primarily, the reliance on large datasets poses challenges in computational resources and efficiency. Indeed, it took about 6 days to run through all of the BookSum values (at least on a low end GPU). The process of generating machine summaries for a vast dataset like BookSum, even with skipping strategies, remains resource-intensive. This limitation calls for future exploration into more efficient algorithms that can maintain or even improve accuracy without the need for extensive computational power.\n",
        "\n",
        "Further, our methodology's success in one domain suggests potential applicability across various other fields. Future research could explore the effectiveness of this approach in different contexts, such as legal text summarization or medical question generation, where accuracy and the quality of information are crucial.\n",
        "\n",
        "In conclusion, this could open up avenues for further research in efficiency and broader applicability. The mixture of AI and languages continues to be a fertile ground for innovation, and projects like this contribute valuable knowledge to this evolving field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l76UEqBeEAkM"
      },
      "source": [
        "# References:\n",
        "[1]:  Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, Dragomir Radev, \"BookSum: A Collection of Datasets for Long-form Narrative Summarization,\" arXiv, 2021. [arXiv:2105.08209]\n",
        "\n",
        "[2]:  Suraj Patil, \"question_generation: An open-source question generation tool using transformer-based models,\" GitHub Repository, 2021. Available: github.com/patil-suraj/question_generation\n",
        "\n",
        "[3]: Liam Dugan, Eleni Miltsakaki, Shriyash Upadhyay, Etan Ginsberg, Hannah Gonzalez, Dayheon Choi, Chuning Yuan, Chris Callison-Burch, \"A Feasibility Study of Answer-Agnostic Question Generation for Education,\" arXiv, 2022."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}